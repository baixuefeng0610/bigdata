map端：
split的数量决定了map的并行度，默认一个block128M就是一个split，split中通过格式化取出每一条记录然后让map读取，根据格式化类，默认是\n换行符，
如果格式化类指定其他分割符，那就是读取的几行记录作为一条记录输入map

reduce端：
在mapreduce计算中，组是一个最小的概念，不能被拆开，reduce的并行度由人为指定。


MR：数据以一条记录为单位经过map方法映射成K-v，相同的key为一组，这一组数据调用一次reduce方法，在方法内迭代计算一组数据。
在reduce方法中，其实并不是传了这一个分组数据，而是传了一个迭代器。mr使用的就是迭代器模式。因此使用迭代器模式。数据在磁盘，迭代器其实就是一个对象，迭代器迭代的时候，数据不在内存，在磁盘，对着文件打开一个io，
包装成一个迭代器，然后next一条一条读取。

在mr缓存区写数据到磁盘的过程是达到阈值后才会写，为啥不计算一条就写一条的呢？
因为jvm程序每计算完一条数据就会写到磁盘，这样会很慢，慢是因为IO会触发调用内核，（cpu本来计算jvm的逻辑代码，但是调用io，要从进程写到磁盘，cpu不会读取jvm的程序代码，会进行用户态到内核态的切换
会进入操作系统的kernal级别，kernal中才是真正的io 的API调用，内核进行将数据刷到磁盘去。所以每写一条数据就会很占资源。）

缓存区shuffe会进行二次排序：第一次是分区排序，第二次是key排序


计算向数据移动：
1、客户端根据每次计算的数据咨询NN元数据，元数据包含block快，然后根据block计算split，得到一个切片清单。就会得到map的数量，block身上有offset，location，由于split和blcok有关系，
因此split包含了偏移量，以及split对应的map任务应该移动到那些节点。可以支持计算向数据移动了
2、生成计算程序未来运行时的配置文件
3、cli将jar和split清单、配置文件上传到hdfs的目录中（上传的数据，副本10）
4、cli会调用jobTracker，通知启动一个计算程序了，且告知文件存放的hdfs文件路径


jobTracker收到启动程序后：
1、从hdfs收回split清单
2、根据自己收到TT汇报的资源，最终确定每一个split对应的map应该去哪一个节点
3、未来TT在心跳的时候，取回分配给自己的任务信息。

TaskTracker：
1、在心跳取回任务后，
2、从hdfs下载jar、xml到本机
3、最终启动任务描述中的MapTask/ReduceTask（最终代码在某一个节点被启动，是通过cli上传，TT下载）


问题：
jobTracker的3个问题：
1、单点故障
2、压力过大
3、集成了资源管理、任务调度两者耦合
4、未来新的框架不能使用，因为各自实现资源管理，但是他们部署在同一批硬件上，因为隔离，所以不能感知对方使用。




yarn模型：
模型：container容器
     虚的：可以吧容易定义为一个对象：对象有：属性、NM，CPU、MEM（内存）、IO
     物理的：1、jvm-操作系统进程（1、NN会有线程监控container资源使用情况，如果超额，直接杀死）
            2、cgroup内核技术：在启动jvm进程，有kernal约束死。
实现：架构
     resourceManager 主
          负责整体资源管理
     NodeManager  从
          向rs汇报心跳，提交自己的资源情况


MR运行 Mapreduce on  yarn
1、MR-cli（切片清单、配置、jar上传到HDFS），访问RM申请appMaster
2、RM选择一台不忙的节点通知NM启动一个Container，在里边映射一个MRAppMaster
3、启动MRAppMaster，从hdfs下载切片清单，向RM申请资源
4、由RM根据自己掌握的资源情况得到一个确定清单，通知NM来启动container
5、container启动后会反向这册到已经启动的MRAppMaster进程
6、MRAppMaster（曾经的JobTracker阉割版不带资源管理）最终将任务Task发送给container（消息）
7、container会反射相应的task类为对象，调用方法执行，其结果就是我们的业务逻辑代码的执行。



java中基本类型是不带序列化和反序列化的方法调用的，因此需要包装成引用类型才可以序列化和反序列化